---
title: "Random landmark time with longitudinal predictors measured at times 1, 2, 3, 4, and 5."
data: March 9, 2022
output: html_document
---

```{r default, include = FALSE, collapse = TRUE}
library(knitr)
opts_chunk$set(prompt = TRUE, comment = "")
```

In the following example, we used the `ranger()` function implemented in `R` package `ranger` (Wright and Ziegler, 2017) to construct survival trees then apply the proposed ensemble method to compute the predicted survival probabilities.

## Generate simulated data

We consider a scenario where the event times are generated from an irreversible multi-state model with three states: healthy, diseased, and death. 
We assume that all subjects started in the healthy state, disease onset is an intermediate event, and death is
the event of interest. 
Suppose we are interested in predicting the survival probability among those who experienced an intermediate event, 
the target landmark probability is $$P(T\ge U + t|T\ge U, U, W(1, U), W(2, U), W(3, U), W(4, U), W(5, U), Z),$$
where $T$ is a continuous failure time, $U$ is the time to the intermediate event (time from the healthy state to the disease state), $W(U)$ is a $q$-dimensional vector of time-dependent predictors measured at times 1, 2, 3, 4, and 5, 
and $Z$ is a $p$-dimensional vector of baseline predictors.


The following codes generate simulated data from `simDat3A()`. 
See vignette on
[generating simulated data](https://htmlpreview.github.io/?https://github.com/stc04003/DynmaicRisk/blob/main/Examples/data.html)
for simulation settings.
```{R}
source("../codes/sim3A.R")
set.seed(1); dat <- simDat3A(n = 400, cen = .2)
```


## Construct trees with `ranger`

The following fits a random survival forest with `ranger()` with a minimum node size of 15.
```{R}
library(survival)
library(ranger)
(fit <- ranger(Surv(Time, status) ~ ., data = dat, min.node.size = 15))
```

## Calculate predicted survival probabilities with the proposed ensemble method

The following codes generate a testing data (`dat0`) and 
compute the predicted probabilities with the proposed ensemble procedure.
See vignette on
[add-on functions for ensemble procedures](https://htmlpreview.github.io/?https://github.com/stc04003/DynmaicRisk/blob/main/Examples/addon.html)
for function descriptions.
```{R}
source("../codes/ranger-addon.R")
dat0 <- simDat3A(500, 0, T)
predS <- getSurv(fit, dat, dat0)
head(predS, 3)
```

The `getSurv()` function returns a list, where the $i$th member of the list gives the landmark probability for 
the $i$th subject from the testing data. 

## Evaluating model performance

Due to the complicated relationship between event times and longitudinal markers, deriving the closed-form expression of
the true probability is challenging. 
The `getTrueSurv3A()` function from `codes/3A.R` compute the true landmark probability using 
Monte Carlo method for this scenario.

```{R}
trueS <- getTrueSurv3A(dat0)
```

The `getTrueSurv3A()` function returns a list, where the $i$th member of the list gives the true landmark probability for 
the $i$th subject from the testing data. 
The following codes calculate the integrated mean absolute error and the integrated mean squared error
to evaluate the model performance. 

```{R}
t0 <- seq(0, quantile(dat0$Time[dat0$status > 0], .9), .01)
## Intergrated absolute error
mean(sapply(1:nrow(dat0), function(i) abs(predS[[i]](t0) - trueS[[i]](t0))))
## Intergrated MSE
mean(sapply(1:nrow(dat0), function(i) (predS[[i]](t0) - trueS[[i]](t0))^2))
```

In addition, the concordance measure, which is equivalent to the area under the ROC curve,
can also be used to evaluate the model performance. 
The cumulative concordance of the testing data can be computed as follows,
with `sc` being the Kaplan-Meier estimate of the censoring distribution.
```{R}
sc <- with(survfit(Surv(Time + D, 1 - status) ~ 1, data = dat), stepfun(time, c(1, surv)))
mean(sapply(t0, getCON, predS, sc, dat0), na.rm = TRUE)
```

## Permutation variable importance

The permutation variable importance of a predictor is computed as the average decrease in 
model accuracy on the out-of-bag (OOB) samples when the respective feature values are randomly permuted.
We extend this idea to study variable importance in dynamic risk prediction using censored data. 

We first calculate the OOB cumulative concordance for prediction based on trees built without 
the $i$th subject and when each predictor of interest is randomly permuted. 
The OOB concordance measure is computed as follows.
```{R}
con0 <- mean(sapply(t0, getCON2, getSi(fit, dat), sc, dat), na.rm = T)
```
The following generates 50 permutations for each predictor
and compute the corresponding OOB cumulative concordance.
```{R}
vnames <- c(paste("Z", 1:10, sep = "."), paste0("W.", 1:10, "."))
B <- 100
```

```{R, cache = TRUE, message = FALSE}
library(mcreplicate)
set.seed(0)
vimps <- sapply(1:length(vnames), function(i) 
  mc_replicate(B, {
    dat2 <- dat
    if (substr(vnames[i], 1, 1) == "Z")
      dat2[,vnames[i]] <- sample(dat2[,vnames[i]])
    else {
      permID <- grep(vnames[i], names(dat2))
      toPerm <- dat2[,permID[1]] > -1e5
      dat2[toPerm, permID] <- dat2[sample(which(toPerm)), permID]
    }
    f <- getSi(fit, dat2)
    mean(sapply(t0, getCON2, f, sc, dat2), na.rm = T)
  }))
```
Define the variable importance as the average difference in OOB concordances over all permutations.
The following plots the variable importance sorts by the median.
```{R, message = FALSE}
library(dplyr)
library(forcats)
library(ggplot2)
dd <- data.frame(vars = rep(vnames, each = B), vimp = con0 - c(vimps))
dd %>% mutate(vars = fct_reorder(vars, vimp, .fun = 'median')) %>% 
  ggplot(aes(x = vars, y = vimp)) + geom_boxplot() + coord_flip()
```


## Reference 

- Wright, M. N. and Ziegler, A. (2017). `ranger`: A fast implementation of random forests for high dimensional
data in `C++` and `R`. Journal of Statistical Software 77 1â€“17.