---
title: "Random landmark time with longitudinal predictors measured at the intermediate event."
data: March 9, 2022
output: html_document
---

```{r default, include = FALSE, collapse = TRUE}
library(knitr)
opts_chunk$set(prompt = TRUE, comment = "")
```

In the following example, we used the `ranger()` function implemented in `R` package `ranger` (Wright and Ziegler, 2017) to construct survival trees then apply the proposed ensemble method to compute the predicted survival probabilities.

## Generate simulated data

We consider a scenario where the event times are generated from an irreversible multi-state model with three states: healthy, diseased, and death. 
We assume that all subjects started in the healthy state, disease onset is an intermediate event, and death is
the event of interest. 
Suppose we are interested in predicting the survival probability among those who experienced an intermediate event, 
the target landmark probability is $$P(T\ge U + t|T\ge U, U, W(U), Z),$$
where $T$ is a continuous failure time, $U$ is the time to the intermediate event (time from the healthy state to the disease state), $W(U)$ is a $q$-dimensional vector of time-dependent predictors measured at $U$, and $Z$ is a $p$-dimensional vector of baseline predictors.

The following codes generate simulated landmark data from the irreversible multi-state model with the intermediate event as the landmark time.
```{R}
Wt <- function(t, a) a * pweibull(.2 * t, 2)
simDat3C <- function(n, cen) {
  p <- 10
  Z <- matrix(rnorm(n * p), n)
  a <- matrix(runif(n * p, -1, 1), n)
  e <- rnorm(n)
  g <- rgamma(n, 2 , 2)
  t1 <- runif(n, 0, 5)
  t2 <- exp(-5 + rowSums(Wt(t1, a[,1:3])) + rowSums(Z[,1:3] * Z[,1:3]) +
            rowSums(Wt(t1, a[,1:3]) * Z[,1:3]) + log(1 + t1) + g + e)
  pb <- 1 / (1 + 1 / exp(rowSums(Wt(t1, a[,1:3])) + rowSums(Z[,1:3]) + g))
  if (cen == 0) cc <- rep(Inf, n)
  if (cen == .2) cc <- rexp(n, .04)
  if (cen == .4) cc <- rexp(n, .12)  
  dat0 <- data.frame(Time = t1 + t2, status = 1, D = t1, Z = Z, W = Wt(t1, a))
  keep <- rbinom(n, 1, pb) > 0
  dat0$Time <- pmin(dat0$Time, cc)
  dat0$status <- 1 * (dat0$Time < cc)
  dat <- subset(dat0[keep,], Time >= D)
  dat$Time <- dat$Time - dat$D
  rownames(dat) <- NULL
  return(dat)
}
set.seed(1); dat <- simDat3C(n = 400, cen = .2)
```

The `simDat3C()` function takes on two arguments, `n` and `cen` for sample size and censoring percentage at the baseline, respectively.
The `simDat3C()` function is also implemented in `codes/sim3C.R` but is included here for the completeness of this vignette.

```{R}
names(dat)
head(dat[, c(1:5, 14:15)]) 
```

The `simDat3C()` returns a `data.frame` with the following variables. 

  - `Time` observed survival time after $U$, e.g., $T-U$.
  - `status` censoring indicator; 1 if `Time` is the event of interest (death) and 0 if `Time` is censored.
  - `D` time from the healthy state to the diseased state. This is also the landmark time in this scenario. 
  - `Z.1`...`Z.10` are 10 baseline predictors.
  - `W.1`...`W.10` are 10 time-dependent predictors measured at $U$.

## Construct trees with `ranger`

The following fits a random survival forest with `ranger()`.
```{R}
library(survival)
library(ranger)
(fit <- ranger(Surv(Time, status) ~ ., data = dat))
```

## Calculate predicted survival probabilities with the proposed ensemble method

The following `getSurv()` implements the proposed ensemble method. 
The proposed ensemble procedure is also included in `getSurv()` from `codes/ranger-addon.R`.
```{R}
getSurv <- function(fit, trainDat, testDat) {
  resChar <- strsplit(as.character(fit$call)[2], "\\s~")[[1]][1] 
  resChar <- gsub("Surv|[(]|[)]", "", resChar)
  resChar <- strsplit(resChar, ", ")[[1]]
  Time <- as.numeric(unlist(trainDat[resChar[1]]))
  Status <- as.numeric(unlist(trainDat[resChar[2]]))
  n <- nrow(trainDat)
  wbij <- sapply(1:fit$forest$num.trees, function(i) {
    out <- tabulate(1 + fit$forest$SampleIDs[[i]], nbins = n)
    return(out) })
  trainDatNodes <- predict(fit, trainDat, type = "terminalNodes")$predictions
  testDatNodes <- predict(fit, testDat, type = "terminalNodes")$predictions
  predSV <- sapply(1:nrow(testDat), function(i) {
    wij <- rowSums(wbij * (testDatNodes[rep(i, nrow(trainDatNodes)),] == trainDatNodes))
    w1 <- sapply(Time, function(x) sum(wij * (Time >= x)))
    exp(-sapply(Time, function(x)
      sum(Status * wij * (Time <= x) / w1, na.rm = TRUE)))
  })
  apply(predSV, 2, function(x) stepfun(sort(Time), c(1, x[order(Time)])))
}
```

The following codes generate a testing data and compute the predicted probabilities with the proposed ensemble procedure.
```{R}
dat0 <- simDat3C(500, 0)
predS <- getSurv(fit, dat, dat0)
head(predS, 3)
```

The `getSurv()` function returns a list, where the $i$th member of the list gives the landmark probability for 
the $i$th subject from the testing data. 

## Evaluating model performance

Due to the complicated relationship between event times and longitudinal markers, 
deriving the closed-form expression of the true probability is challenging. 
Thus, we rely on empirical method to compute the true landmark probability using 
Monte Carlo method for this scenario.
The method is implemented as `getTrueSurv3C()`, which is also included in `codes/3C.R`.

```{R}
getTrueSurv3C <- function(dat) {
  trSurv <- list()
  n2 <- 1e5
  for (i in 1:nrow(dat)) {
    W0 <- as.numeric(dat[i, grep("W", names(dat))])[1:3]
    Z0 <- as.numeric(dat[i, grep("Z", names(dat))])[1:3]    
    t1 <- as.numeric(dat$D[i])
    g <- rgamma(n2, 2, 2)
    pb2 <- 1 / (1 + 1 / exp(sum(W0) + sum(Z0) + g))
    pi2 <- rbinom(n2, 1, pb2)
    t22 <- exp(-5 + sum(W0) + sum(Z0^2) + sum(W0 * Z0) + log(1 + t1) + g + rnorm(n2))
    t22 <- sort(t22[pi2 > 0])
    trSurv[[i]] <- stepfun(t22, c(1, 1 - ecdf(t22)(t22)))
  }
  trSurv
}
trueS <- getTrueSurv3C(dat0)
```

The `getTrueSurv3C()` function returns a list, where the $i$th member of the list gives the true landmark probability for 
the $i$th subject from the testing data. 
The following codes calculate the integrated mean absolute error and the integrated mean squared error
to evaluate the model performance. 

```{R}
t0 <- seq(0, quantile(dat0$Time[dat0$status > 0], .9), .01)
## Intergrated absolute error
mean(sapply(1:nrow(dat0), function(i) abs(predS[[i]](t0) - trueS[[i]](t0))))
```

```{R}
## Intergrated MSE
mean(sapply(1:nrow(dat0), function(i) (predS[[i]](t0) - trueS[[i]](t0))^2))
```

In addition, the concordance measure, which is equivalent to the area under the ROC curve,
can also be used to evaluate the model performance. 
The following implementation can be used to compute the concordance at time `tt`. 
```{R}
getCON <- function(tt, si, sc, df0) {
  c1 <- outer(df0$status, rep(1, length(df0$status)))
  c2 <- outer(df0$Time <= tt, df0$Time > tt)
  l <- length(si)
  stmp <- sapply(1:l, function(x) si[[x]](tt))
  c3 <- 1 * outer(stmp, stmp, "<") + 0.5 * outer(stmp, stmp, "==")
  c4 <- outer(sc(df0$Time + df0$D), sc(df0$Time + tt))
  c4 <- ifelse(c4 < 1e-5, NA, c4)
  sum(c1 * c2 * c3 / c4, na.rm = TRUE) / sum(c1 * c2 / c4, na.rm = TRUE)
}
```
The cumulative concordance of the testing data can be computed as follows,
with `sc` being the Kaplan-Meier estimate of the censoring distribution.
```{R}
sc <- with(survfit(Surv(Time + D, 1 - status) ~ 1, data = dat), stepfun(time, c(1, surv)))
mean(sapply(t0, getCON, predS, sc, dat0), na.rm = TRUE)
```

## Permutation variable importance

The permutation variable importance of a predictor is computed as the average decrease in 
model accuracy on the out-of-bag (OOB) samples when the respective feature values are randomly permuted.
We extend this idea to study variable importance in dynamic risk prediction using censored data. 
First, we observe the cumulative concordance of the training data as follows. 
```{R}
(con0 <- mean(sapply(t0, getCON, getSurv(fit, dat, dat), sc, dat), na.rm = T))
```
Then, we calculate the OOB cumulative concordance for prediction based on trees built without 
the $i$th subject and when each predictor of interest is randomly permuted. 
The following implemention gives a survival prediction without the $i$th subject.
```{R}
getSi <- function(fit, df) {
  df <- df[order(df$Time + df$status),]
  tt <- sort(df$Time)
  rownames(df) <- NULL
  sapply(1:nrow(df), function(s) {
    fit.tmp <- fit
    takei <- which(unlist(lapply(fit$forest$OobSampleIDs, function(x) any(x %in% (s - 1)))))
    fit.tmp$forest$OobSampleIDs <- fit.tmp$forest$OobSampleIDs[takei]
    fit.tmp$forest$num.trees <- fit.tmp$num.trees <- length(takei)
    fit.tmp$forest$child.nodeIDs <- fit.tmp$forest$child.nodeIDs[takei]
    fit.tmp$forest$split.values <- fit.tmp$forest$split.values[takei]
    fit.tmp$forest$SampleIDs <- fit.tmp$forest$SampleIDs[takei]
    fit.tmp$forest$chf <- fit.tmp$forest$chf[takei]
    fit.tmp$forest$split.varIDs <- fit.tmp$forest$split.varIDs[takei]
    getSurv(fit.tmp, df, df[s,])
  })
}
```
The following generates 100 permutations for each predictor $Z_1, \ldots, Z_{10}, W_1(U), \ldots, W_{10}(U)$ 
and compute the corresponding OOB cumulative concordance.
```{R}
vnames <- setdiff(names(dat), c("Time", "status", "D"))
B <- 100
```

```{R, cache = TRUE, message = FALSE}
set.seed(0)
library(mcreplicate)
vimps <- sapply(1:length(vnames), function(i) 
  mc_replicate(B, {
    dat2 <- dat[order(dat$Time),]
    dat2[,vnames[i]] <- ifelse(dat2$status > 0, sample(dat2[,vnames[i]]), dat2[,vnames[i]])
    f <- getSi(fit, dat2)
    mean(sapply(t0, getCON, f, sc, dat2), na.rm = T)}))
```
The following plots the variable importances sorts by the median OOB cumulative concordance.
```{R, message = FALSE}
library(dplyr)
library(forcats)
library(ggplot2)
dd <- data.frame(vars = rep(vnames, each = B), vimp = con0 - c(vimps))
dd %>% mutate(vars = fct_reorder(vars, vimp, .fun = 'median')) %>% 
  ggplot(aes(x = vars, y = vimp)) + geom_boxplot() + coord_flip()
```

<!--The considered example is the Scenario (III-C) in our manuscript. 
The functions `simDat()` and `getTrueSurv()` are also available in `../codes/sim3C.R`.
The function `getSurv()` is also available in `../codes/ranger-addon.R`.-->


## Reference 

- Wright, M. N. and Ziegler, A. (2017). `ranger`: A fast implementation of random forests for high dimensional
data in `C++` and `R`. Journal of Statistical Software 77 1–17.